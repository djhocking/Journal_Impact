This is an interesting exploration of the ideas behind, and relationships between, various measures of journal level citation based impact.

I felt that the analysis could have been taken a step further by considering the multivariate relationships between factors using principle components analysis or a more advanced variation (e.g., NMDS). This would have taken the empirical analysis beyond other similar analyses (e.g., http://sci2s.ugr.es/hindex/pdf/Bornmannetal2009.pdf). This shouldn't be required for publication, but it would definitely improve the general interest in the paper and shouldn't require too much effort.

Figure 1 and associated results. I'm not an expert on this, but the idea of mixing Pearson and Spearman correlations seems strange since it is going to invite readers to compare two different metrics as is they were the same. If it was me I would have probably either used Spearman for everything or used an adjusted R^2 about the spline (assuming that there is such a thing).

The discussion of DORA seems to mis-communicate the core point. The central argument made in DORA is not against metrics per se (though it does encourage more thorough forms of evaluation), but against using the metrics associated with the journal that the research is published in as a way of evaluating researchers. This is clearly bad practice (since highly impactful work can be published in journals with low impact metrics) as has been laid out even by the folks generating the JIF. So, while metrics can be useful in researcher evaluation, journal level metrics are at best very poor surrogates for what we are interested in and should be abandoned for this purpose. I think this point should be made clearly in all bibliometric analyses to help reduce the clearly inappropriate use of these metrics in researcher evaluation.

Table 2 lists the background trends as unknown for AI and Eigenfactor. Information on trends in these metrics is available at the Eigenfactor site. Go to 'eigenfactor search' -> 'motion graphs', select Ecology and Evolution from the dropdown  and then select the trend option (picture of time series, from the upper right hand corner). Eigenfactor appears to have a positive trend. Article influence seems to be pretty flat. Raw data could also be quite quickly sampled from the journal level reports. This certainly isn't crucial information to the overall ms, but if it's worth providing for the other metrics then I think it's worth a little more work to provide it for these two as well.

Minor comments
--------------

(Note, given that the manuscript is provided as a Word document without line numbers, and I am a Linux user, I cannot be sure that the line numbers I am providing match those that the author will see. I would be happy to send along a pdf of what the ms looks like on my machine if these do not line up properly)

Abstract. First sentence. Delete "tremendous".
* Introduction. Line 3. "aggregate" not "aggregation"
* Introduction. * Line 8. "...rejection rate TRADE-OFF AGAINST the benefit..."
* Introduction. * Line 10. Delete "being"
* Introduction. * Line 11. Instead of saying that this is to the chagrin of researchers I would say that this is explicitly recommended against. As it is currently phrased it does not communicate that there are good solid arguments against doing so. Even the creators of the impact factor explicitly recommend against this use. A citation from the IF folks should be tracked down and included.
* Introduction. * Start of paragraph 2. I'm pretty sure that JCR isn't "freely available". The library at the authors institution may subscribe to it, but I am unable to access it from home.
* Introduction. * End of second paragraph. There should be an "and" before "10) ease..."
* Results. Last sentence. "I estimated means of..."
* Discussion. 1st paragraph. The manuscript points out that review articles are more highly cited than regular papers and that this influences measures of article influence. However it fails to note (as we point out in Supp & White 2010) that one of the reasons that Ecology Letters scores so highly on citation metrics is that it publishes a much higher proportion of review articles than other research focused ecology journals.
* Discussion. 2nd paragraph. Last sentence. "differences in the databases RATHER than DIFFERENCES between the" or something else, but as the moment it doesn't quite work.
* Discussion. Page 13. The mention of a "greater number of short articles" assumes limited page space. While this is a reality for some journals it is increasingly less common in the age of digital only publishing. It may be simpler to remove the word "short".
* Discussion. Page 14. Last sentence. This strikes me as an overly broad statement. Besides that fact that all else is never going to be equal (e.g., it seems likely there will be a clear tradeoff if a journal publishes too much non-impactful work where the work in the journal on average will get less attention), assuming that journals are any good at identifying the most impactful papers we wouldn't necessarily expect h related indices to be influenced by publishing more. This is clearly true in the presence of perfect prediction of impact as adding more papers will by definition add only papers that do not contribute to have due to their lower citation levels.
* Page 17. Middle paragraph. Do you mean "journal bundles" not "database bundles"
* Discussion, final paragraph. DORA quote. I'm not sure what you're indicating with the "[sic]" here. That is a valid spelling https://en.wiktionary.org/wiki/licence.

Ethan White (I am intentionally signing my review)
